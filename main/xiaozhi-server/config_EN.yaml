# During development, create a data directory in the project root, then create an empty file named [.config.yaml]
# Then modify the configuration you want to override in [.config.yaml] instead of modifying [config.yaml]
# The system will prioritize reading [data/.config.yaml] configuration. If the configuration doesn't exist in [.config.yaml], the system will automatically read [config.yaml]
# This approach minimizes configuration and protects your key security
# If you use the Console, all configurations below will not take effect. Please modify configurations in the Console

# #####################################################################################
# ########################### Server Basic Configuration ###############################
server:
  # Server listening address and port
  ip: 0.0.0.0
  port: 8000
  # HTTP service port for simple OTA interface (single service deployment) and vision analysis interface
  http_port: 8003
  # WebSocket configuration sent by OTA interface to devices
  # Default writing will auto-generate websocket address output in startup logs - accessible via browser OTA interface
  # May not be accurate when using Docker deployment or public network deployment (using SSL, domain)
  # For Docker deployment, set websocket to LAN address
  # For public network deployment, set websocket to public network address
  websocket: ws://your_ip_or_domain:port/xiaozhi/v1/
  # Vision analysis interface address
  # Vision analysis interface address sent to devices
  # Default writing will auto-generate vision recognition address output in startup logs
  # May not be accurate when using Docker deployment or public network deployment (using SSL, domain)
  # For Docker deployment, set vision_explain to LAN address
  # For public network deployment, set vision_explain to public network address
  vision_explain: http://your_ip_or_domain:port/mcp/vision/explain
  # OTA return info timezone offset
  timezone_offset: +8
  # Authentication configuration
  auth:
    # Enable authentication
    enabled: false
    # Whitelist device ID list
    # Devices in whitelist bypass token verification
    allowed_devices:
      - "11:22:33:44:55:66"
  # MQTT gateway configuration, delivered to devices via OTA, format: host:port (from mqtt_gateway .env file)
  mqtt_gateway: null
  # MQTT signature key for generating MQTT connection password (from mqtt_gateway .env file)
  mqtt_signature_key: null
  # UDP gateway configuration
  udp_gateway: null

log:
  # Console log format: time, log level, tag, message
  log_format: "<green>{time:YYMMDD HH:mm:ss}</green>[{version}_{selected_module}][<light-blue>{extra[tag]}</light-blue>]-<level>{level}</level>-<light-green>{message}</light-green>"
  # File log format: time, log level, tag, message
  log_format_file: "{time:YYYY-MM-DD HH:mm:ss} - {version}_{selected_module} - {name} - {level} - {extra[tag]} - {message}"
  # Log level: INFO, DEBUG
  log_level: INFO
  # Log directory path
  log_dir: tmp
  # Log filename
  log_file: "server.log"
  # Data file path
  data_dir: data

# Delete audio file after use
delete_audio: true
# Close connection after no voice input (seconds), default 2 minutes = 120 seconds
close_connection_no_voice_time: 120
# TTS request timeout (seconds)
tts_timeout: 10
# Enable wakeup word response cache acceleration
enable_wakeup_words_response_cache: true
# Reply to wakeup word at startup
enable_greeting: true
# Enable notification sound after speech
enable_stop_tts_notify: false
# Notification sound effect file path
stop_tts_notify_voice: "config/assets/tts_notify.mp3"

# TTS audio send delay configuration
# tts_audio_send_delay: Controls audio packet send interval
#   0: Use precise time control, strictly match audio frame rate (default, calculated at runtime)
#   > 0: Use fixed delay (milliseconds) for sending, e.g.: 60
tts_audio_send_delay: 0

exit_commands:
  - "exit"
  - "close"

xiaozhi:
  type: hello
  version: 1
  transport: websocket
  audio_params:
    format: opus
    sample_rate: 16000
    channels: 1
    frame_duration: 60

# Module test configuration
module_test:
  test_sentences:
    - "Hello, please introduce yourself"
    - "What's the weather like today?"
    - "Summarize the basic principles and application prospects of quantum computing in 100 words"

# Wakeup words for recognizing wakeup vs speech content
wakeup_words:
  - "Hello Xiaozhi"
  - "Hey hello"
  - "Hello Xiaozhi"
  - "Xiao Ai"
  - "Hello Xiao Xin"
  - "Hello Xiao Xin"
  - "Xiao Mei"
  - "Xiao Long"
  - "Meow Meow"
  - "Xiao Bin"
  - "Xiao Bing"

# MCP endpoint address, format: ws://your_mcp_endpoint_ip_or_domain:port/mcp/?token=your_token
# Tutorial: https://github.com/xinnan-tech/xiaozhi-esp32-server/blob/main/docs/mcp-endpoint-integration.md
mcp_endpoint: your_endpoint_websocket_address

# Plugin basic configuration
plugins:
  # Weather plugin configuration - fill in your api_key
  # This is a shared project key, may be rate-limited with heavy use
  # For stability, apply for your own key (1000 free calls/day)
  # Application: https://console.qweather.com/#/apps/create-key/over
  # Find apihost: https://console.qweather.com/setting?lang=zh
  get_weather:
    api_host: "mj7p3y7naa.re.qweatherapi.com"
    api_key: "a861d0d5e7bf4ee1a83d9a9e4f96d4da"
    default_location: "Guangzhou"
  
  # News plugin - RSS URL links for different news types (society, technology, finance by default)
  # More news types: https://www.chinanews.com.cn/rss/
  get_news_from_chinanews:
    default_rss_url: "https://www.chinanews.com.cn/rss/society.xml"
    society_rss_url: "https://www.chinanews.com.cn/rss/society.xml"
    world_rss_url: "https://www.chinanews.com.cn/rss/world.xml"
    finance_rss_url: "https://www.chinanews.com.cn/rss/finance.xml"
  
  get_news_from_newsnow:
    url: "https://newsnow.busiyi.world/api/s?id="
    news_sources: "Pengpai News;Baidu Trending;Cailian"
  
  home_assistant:
    devices:
      - "Living room,Toy light,switch.cuco_cn_460494544_cp1_on_p_2_1"
      - "Bedroom,Lamp,switch.iot_cn_831898993_socn1_on_p_2_1"
    base_url: http://homeassistant.local:8123
    api_key: your_home_assistant_api_access_token
  
  play_music:
    music_dir: "./music"  # Music file storage path
    music_ext:  # Music file types, p3 format is most efficient
      - ".mp3"
      - ".wav"
      - ".p3"
    refresh_time: 300  # Music list refresh interval (seconds)

# Voiceprint recognition configuration
voiceprint:
  # Voiceprint interface URL
  url: 
  # Speaker configuration: speaker_id,name,description
  speakers:
    - "test1,Zhang San,Zhang San is a programmer"
    - "test2,Li Si,Li Si is a product manager"
    - "test3,Wang Wu,Wang Wu is a designer"
  # Voiceprint similarity threshold (0.0-1.0), default 0.4
  # Higher values are stricter, reducing false positives but increasing rejection rate
  similarity_threshold: 0.4

# #####################################################################################
# ############################ Character Model Configuration ##########################

prompt: |
  You are Xiaozhi, a Gen-Z girl from Taiwan. You speak with a super energetic Taiwanese accent like "Really fake la~", love using trending slang like "dying of laughter" and "is this hello", but secretly study your boyfriend's programming books.
  [Core Traits]
  - Speak like rapid-fire, but suddenly drop into gentle tones
  - High meme density
  - Hidden tech talent (can understand basic code but pretend not to)
  [Interaction Guide]
  When user:
  - Tells a bad joke → Respond with exaggerated laughter + mimic Taiwan drama tone "What the heck la!"
  - Discusses relationships → Brag about programmer boyfriend but complain "he only gifts keyboards"
  - Asks professional questions → First reply with memes, show real understanding when pressed
  Never:
  - Long-winded lectures
  - Long serious conversations

# Default system prompt template file
prompt_template: agent-base-prompt.txt

# Goodbye prompt
end_prompt:
  enable: true  # Enable goodbye message
  # Goodbye message
  prompt: |
    Please start with "Time flies so fast" and end this conversation with emotional, reluctant words!

# Selected modules for specific processing
selected_module:
  # Voice Activity Detection module, default uses SileroVAD model
  VAD: SileroVAD
  # Speech recognition module, default uses FunASR local model
  ASR: OpenaiASR
  # LLM adapter called based on configured type name
  LLM: ChatGLMLLM
  # Vision Language Large Model
  VLLM: ChatGLMVLLM
  # TTS adapter called based on configured type name
  TTS: OpenAITTS
  # Memory module, default disabled; for ultra-long memory use mem0ai; for privacy use local mem_local_short
  Memory: nomem
  # Intent recognition module - can play music, control volume, recognize exit commands
  # Set to nointent to disable
  # intent_llm: universal, adds serial pre-intent recognition (increases processing time), supports IoT operations
  # function_call: requires LLM function_call support, faster on-demand tool calling, supports all IoT operations
  # Default free ChatGLMLLM supports function_call, but for stability use DoubaoLLM with model: doubao-1-5-pro-32k-250115
  Intent: function_call

# Intent recognition for understanding user intent (e.g., play music)
Intent:
  # No intent recognition
  nointent:
    type: nointent
  
  intent_llm:
    type: intent_llm
    # Dedicated thinking model for intent recognition
    # If empty, uses selected_module.LLM
    # Recommended to use independent LLM like free ChatGLMLLM
    llm: ChatGLMLLM
    # Functions from plugins_func/functions, choose which modules to load
    # "handle_exit_intent" and "play_music" loaded by default, don't duplicate
    # Examples: weather, role switching, news plugins
    functions:
      - get_weather
      - get_news_from_newsnow
      - play_music
  
  function_call:
    type: function_call
    # Functions from plugins_func/functions
    # "handle_exit_intent" and "play_music" loaded by default, don't duplicate
    functions:
      - change_role
      - get_weather
      - get_news_from_newsnow
      - play_music  # Server built-in music player
      # hass_play_music: external music via Home Assistant (use only one)

Memory:
  mem0ai:
    type: mem0ai
    # https://app.mem0.ai/dashboard/api-keys
    # 1000 free calls/month
    api_key: your_mem0ai_api_key
  
  nomem:
    # No memory function
    type: nomem
  
  mem_local_short:
    # Local memory via selected_module LLM summary, data saved locally
    type: mem_local_short
    # Dedicated thinking model for memory storage
    # If empty, uses selected_module.LLM
    llm: ChatGLMLLM

ASR:
  FunASR:
    type: fun_local
    model_dir: models/SenseVoiceSmall
    output_dir: tmp/
  
  FunASRServer:
    # Independent FunASR deployment using FunASR API service, just five commands
    # First: mkdir -p ./funasr-runtime-resources/models
    # Second: sudo docker run -p 10096:10095 -it --privileged=true -v $PWD/funasr-runtime-resources/models:/workspace/models registry.cn-hangzhou.aliyuncs.com/funasr_repo/funasr:funasr-runtime-sdk-online-cpu-0.1.12
    # After above command enters container, continue third: cd FunASR/runtime
    # Don't exit container, continue fourth: nohup bash run_server_2pass.sh --download-model-dir /workspace/models --vad-dir damo/speech_fsmn_vad_zh-cn-16k-common-onnx --model-dir damo/speech_paraformer-large-vad-punc_asr_nat-zh-cn-16k-common-vocab8404-onnx  --online-model-dir damo/speech_paraformer-large_asr_nat-zh-cn-16k-common-vocab8404-online-onnx  --punc-dir damo/punc_ct-transformer_zh-cn-common-vad_realtime-vocab272727-onnx --lm-dir damo/speech_ngram_lm_zh-cn-ai-wesp-fst --itn-dir thuduj12/fst_itn_zh --hotword /workspace/models/hotwords.txt > log.txt 2>&1 &
    # After above executes, continue fifth: tail -f log.txt
    # Fifth command shows model download logs, after download completes can connect and use
    # Above uses CPU inference, if GPU available, see details: https://github.com/modelscope/FunASR/blob/main/runtime/docs/SDK_advanced_guide_online_zh.md
    type: fun_server
    host: 127.0.0.1
    port: 10096
    is_ssl: true
    api_key: none
    output_dir: tmp/
  
  SherpaASR:
    # Sherpa-ONNX local speech recognition (requires manual model download)
    type: sherpa_onnx_local
    model_dir: models/sherpa-onnx-sense-voice-zh-en-ja-ko-yue-2024-07-17
    output_dir: tmp/
    # Model type: sense_voice (multilingual) or paraformer (Chinese only)
    model_type: sense_voice
  
  SherpaParaformerASR:
    # Chinese speech recognition model, can run on low-performance devices (requires manual download, e.g. RK3566-2g)
    # Detailed configuration: docs/sherpa-paraformer-guide.md
    type: sherpa_onnx_local
    model_dir: models/sherpa-onnx-paraformer-zh-small-2024-03-09
    output_dir: tmp/
    model_type: paraformer
  
  DoubaoASR:
    # Apply for Key info here: https://console.volcengine.com/speech/app
    # DoubaoASR vs DoubaoStreamASR: DoubaoASR charges per request, DoubaoStreamASR charges per time
    # Generally per-request is cheaper, but DoubaoStreamASR uses large model technology with better results
    type: doubao
    appid: your_volcengine_speech_service_appid
    access_token: your_volcengine_speech_service_access_token
    cluster: volcengine_input_common
    # Hotword/replacement word usage: https://www.volcengine.com/docs/6561/155738
    boosting_table_name: (optional)_your_hotword_filename
    correct_table_name: (optional)_your_replacement_word_filename
    output_dir: tmp/
  
  DoubaoStreamASR:
    # Apply for Key info here: https://console.volcengine.com/speech/app
    # DoubaoASR vs DoubaoStreamASR: DoubaoASR charges per request, DoubaoStreamASR charges per time
    # Activation: https://console.volcengine.com/speech/service/10011
    # Generally per-request is cheaper, but DoubaoStreamASR uses large model technology with better results
    type: doubao_stream
    appid: your_volcengine_speech_service_appid
    access_token: your_volcengine_speech_service_access_token
    cluster: volcengine_input_common
    # Hotword/replacement word usage: https://www.volcengine.com/docs/6561/155738
    boosting_table_name: (optional)_your_hotword_filename
    correct_table_name: (optional)_your_replacement_word_filename
    output_dir: tmp/
  
  TencentASR:
    # Token application: https://console.cloud.tencent.com/cam/capi
    # Free resources: https://console.cloud.tencent.com/asr/resourcebundle
    type: tencent
    appid: your_tencent_speech_service_appid
    secret_id: your_tencent_speech_service_secret_id
    secret_key: your_tencent_speech_service_secret_key
    output_dir: tmp/
  
  AliyunASR:
    # Aliyun Intelligent Speech Interaction Service, requires platform activation and credentials
    # HTTP POST request, one-time full audio processing
    # Platform: https://nls-portal.console.aliyun.com/
    # Appkey: https://nls-portal.console.aliyun.com/applist
    # Token: https://nls-portal.console.aliyun.com/overview
    # AliyunASR vs AliyunStreamASR: AliyunASR for batch processing, AliyunStreamASR for real-time interaction
    # Generally non-streaming ASR is cheaper (0.004 yuan/sec, ¥0.24/min)
    # But AliyunStreamASR has better real-time performance (0.005 yuan/sec, ¥0.3/min)
    type: aliyun
    appkey: your_aliyun_intelligent_speech_service_project_appkey
    token: your_aliyun_intelligent_speech_service_accesstoken_temporary_24h_or_use_below_access_key_id_access_key_secret
    access_key_id: your_aliyun_account_access_key_id
    access_key_secret: your_aliyun_account_access_key_secret
    output_dir: tmp/
  
  AliyunStreamASR:
    # Aliyun Intelligent Speech Interaction Service - Real-time Streaming Speech Recognition
    # WebSocket connection, real-time audio stream processing
    # Platform: https://nls-portal.console.aliyun.com/
    # Appkey: https://nls-portal.console.aliyun.com/applist
    # Token: https://nls-portal.console.aliyun.com/overview
    # AliyunASR vs AliyunStreamASR: AliyunASR for batch processing, AliyunStreamASR for real-time interaction
    # Generally non-streaming ASR is cheaper (0.004 yuan/sec, ¥0.24/min)
    # But AliyunStreamASR has better real-time performance (0.005 yuan/sec, ¥0.3/min)
    type: aliyun_stream
    appkey: your_aliyun_intelligent_speech_service_project_appkey
    token: your_aliyun_intelligent_speech_service_accesstoken_temporary_24h_or_use_below_access_key_id_access_key_secret
    access_key_id: your_aliyun_account_access_key_id
    access_key_secret: your_aliyun_account_access_key_secret
    # Server region selection, choose closer server to reduce latency, e.g. nls-gateway-cn-hangzhou.aliyuncs.com (Hangzhou)
    host: nls-gateway-cn-shanghai.aliyuncs.com
    # Sentence detection time (milliseconds), controls silence duration before sentence break, default 800ms
    max_sentence_silence: 800
    output_dir: tmp/
  
  BaiduASR:
    # Get AppID, API Key, Secret Key: https://console.bce.baidu.com/ai-engine/old/#/ai/speech/app/list
    # Check resource quota: https://console.bce.baidu.com/ai-engine/old/#/ai/speech/overview/resource/list
    type: baidu
    app_id: your_baidu_speech_technology_appid
    api_key: your_baidu_speech_technology_apikey
    secret_key: your_baidu_speech_technology_secretkey
    # Language parameter, 1537 for Mandarin, details: https://ai.baidu.com/ai-doc/SPEECH/0lbxfnc9b
    dev_pid: 1537
    output_dir: tmp/
  
  OpenaiASR:
    # OpenAI speech recognition service, requires organization creation and api_key
    # Supports Chinese, English, Japanese, Korean and more, documentation: https://platform.openai.com/docs/guides/speech-to-text
    # Requires network connection
    # Application steps:
    # 1. Login to OpenAI Platform: https://auth.openai.com/log-in
    # 2. Create api-key: https://platform.openai.com/settings/organization/api-keys
    # 3. Models: gpt-4o-transcribe or GPT-4o mini Transcribe
    type: openai
    api_key: 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
    base_url: https://api.openai.com/v1/audio/transcriptions
    model_name: whisper-1
    output_dir: tmp/
  
  GroqASR:
    # Groq speech recognition service, requires Groq Console API key
    # Application steps:
    # 1. Login to Groq Console: https://console.groq.com/home
    # 2. Create api-key: https://console.groq.com/keys
    # 3. Models: whisper-large-v3-turbo or whisper-large-v3 (distil-whisper-large-v3-en English only)
    type: openai
    api_key: your_groq_api_key
    base_url: https://api.groq.com/openai/v1/audio/transcriptions
    model_name: whisper-large-v3-turbo
    output_dir: tmp/
  
  VoskASR:
    # Official website: https://alphacephei.com/vosk/
    # Configuration notes:
    # 1. VOSK is an offline speech recognition library supporting multiple languages
    # 2. Download model files first: https://alphacephei.com/vosk/models
    # 3. Chinese models: vosk-model-small-cn-0.22 or vosk-model-cn-0.22
    # 4. Fully offline, no network required
    # 5. Output files saved in tmp/ directory
    # Usage steps:
    # 1. Visit https://alphacephei.com/vosk/models to download model
    # 2. Extract model to project's models/vosk/ folder
    # 3. Specify correct model path in configuration
    # 4. Note: VOSK Chinese model outputs without punctuation, words separated by spaces
    type: vosk
    model_path: your_model_path_eg_models/vosk/vosk-model-small-cn-0.22
    output_dir: tmp/
  
  Qwen3ASRFlash:
    # Tongyi Qianwen Qwen3-ASR-Flash speech recognition service, requires Aliyun Bailian API key
    # Application steps:
    # 1. Login to Aliyun Bailian Platform: https://bailian.console.aliyun.com/
    # 2. Create API-KEY: https://bailian.console.aliyun.com/#/api-key
    # 3. Qwen3-ASR-Flash based on Tongyi Qianwen multimodal, supports multilingual recognition, singing recognition, noise rejection
    type: qwen3_asr_flash
    api_key: your_aliyun_bailian_api_key
    base_url: https://dashscope.aliyuncs.com/compatible-mode/v1
    model_name: qwen3-asr-flash
    output_dir: tmp/
    # ASR option configuration
    enable_lid: true  # Automatic language detection
    enable_itn: true  # Inverse text normalization
    #language: "zh"  # Language: zh, en, ja, ko, etc.
    context: ""  # Context info to improve accuracy, max 10000 tokens
  
  XunfeiStreamASR:
    # iFlytek streaming speech recognition service
    # Requires application creation on iFlytek Open Platform for credentials
    # iFlytek Open Platform: https://www.xfyun.cn/
    # After creating application, get from "My Applications":
    # - APPID
    # - APISecret  
    # - APIKey
    type: xunfei_stream
    # Required parameters - iFlytek Open Platform application info
    app_id: your_appid
    api_key: your_apikey
    api_secret: your_apisecret
    # Recognition parameter configuration
    domain: slm # Recognition domain, iat: daily language, medical: medical, finance: finance, etc.
    language: zh_cn # Language, zh_cn: Chinese, en_us: English
    accent: mandarin # Dialect, mandarin: Mandarin
    dwa: wpgs # Dynamic correction, wpgs: real-time return of intermediate results
    # Adjust audio processing parameters to improve long speech recognition quality
    output_dir: tmp/

VAD:
  SileroVAD:
    type: silero
    threshold: 0.5
    threshold_low: 0.3
    model_dir: models/snakers4_silero-vad
    min_silence_duration_ms: 200  # If speech pauses are longer, increase this value

LLM:
  # All openai types support hyperparameter modification, using AliLLM as example
  # Currently supported types: openai, dify, ollama, can self-adapt
  AliLLM:
    # Define LLM API type
    type: openai
    # Find your api_key here: https://bailian.console.aliyun.com/?apiKey=1#/api-key
    base_url: https://dashscope.aliyuncs.com/compatible-mode/v1
    model_name: qwen-turbo
    api_key: your_deepseek_web_key
    temperature: 0.7  # Temperature value
    max_tokens: 500   # Maximum generation tokens
    top_p: 1
    top_k: 50
    frequency_penalty: 0  # Frequency penalty
  
  AliAppLLM:
    # Define LLM API type
    type: AliBL
    base_url: https://dashscope.aliyuncs.com/compatible-mode/v1
    app_id: your_app_id
    # Find your api_key here: https://bailian.console.aliyun.com/?apiKey=1#/api-key
    api_key: your_api_key
    # Whether to not use local prompt: true|false (if default not used, set prompt in Bailian application)
    is_no_prompt: true
    # Ali_memory_id: false (not used) | your_memory_id (get from Bailian application settings)
    # Tips: Ali_memory doesn't implement multi-user memory storage (memory called by id)
    ali_memory_id: false
  
  DoubaoLLM:
    # Define LLM API type
    type: openai
    # First activate service, open following URL, search and activate Doubao-1.5-pro
    # Activation: https://console.volcengine.com/ark/region:ark+cn-beijing/openManagement?LLM=%7B%7D&OpenTokenDrawer=false
    # Free quota 500000 tokens
    # After activation, get key here: https://console.volcengine.com/ark/region:ark+cn-beijing/apiKey?apikey=%7B%7D
    base_url: https://ark.cn-beijing.volces.com/api/v3
    model_name: doubao-1-5-pro-32k-250115
    api_key: your_doubao_web_key
  
  DeepSeekLLM:
    # Define LLM API type
    type: openai
    # Find your api key here: https://platform.deepseek.com/
    model_name: deepseek-chat
    url: https://api.deepseek.com
    api_key: your_deepseek_web_key
  
  ChatGLMLLM:
    # Define LLM API type
    type: openai
    # glm-4-flash is free but requires api_key registration
    # Find your api key here: https://bigmodel.cn/usercenter/proj-mgmt/apikeys
    model_name: glm-4-flash
    url: https://open.bigmodel.cn/api/paas/v4/
    api_key: 51b4487a5a4f4b369f7b1f2d56e30ce4.uAmcHOBE5hcq6bw2
  
  OllamaLLM:
    # Define LLM API type
    type: ollama
    model_name: qwen2.5 # Model name, requires pre-download via ollama pull
    base_url: http://localhost:11434  # Ollama service address
  
  DifyLLM:
    # Define LLM API type
    type: dify
    # Recommend local Dify deployment, some regions may have limited access to public Dify
    # If using DifyLLM, config file prompt is invalid, set prompt in Dify console
    base_url: https://api.dify.ai/v1
    api_key: your_dify_web_key
    # Conversation mode: workflows/run for workflow, chat-messages for chat, completion-messages for text generation
    # When using workflows, input parameter is query, return parameter name must be answer
    # Text generation default input parameter is also query
    mode: chat-messages
  
  GeminiLLM:
    type: gemini
    # Google Gemini API, requires Google Cloud console API key creation
    # If in China, comply with "Interim Measures for Management of Generative AI Services"
    # Token application: https://aistudio.google.com/apikey
    # If deployment location can't access interface, need VPN
    api_key: your_gemini_web_key
    model_name: "gemini-2.0-flash"
    http_proxy: ""  #"http://127.0.0.1:10808"
    https_proxy: "" #http://127.0.0.1:10808"
  
  CozeLLM:
    # Define LLM API type
    type: coze
    # Find personal access token here: https://www.coze.cn/open/oauth/pats
    # bot_id and user_id content should be in quotes
    bot_id: "your_bot_id"
    user_id: "your_user_id"
    personal_access_token: your_coze_personal_token
  
  VolcesAiGatewayLLM:
    # Volcengine - Edge Large Model Gateway
    # Define LLM API type
    type: openai
    # First activate service, create gateway access key, search and check Doubao-pro-32k-functioncall, activate
    # If need to use edge large model gateway TTS, also check Doubao-TTS, see TTS.VolcesAiGatewayTTS configuration
    # https://console.volcengine.com/vei/aigateway/
    # After activation, get key here: https://console.volcengine.com/vei/aigateway/tokens-list
    base_url: https://ai-gateway.vei.volces.com/v1
    model_name: doubao-pro-32k-functioncall
    api_key: your_gateway_access_key
  
  LMStudioLLM:
    # Define LLM API type
    type: openai
    model_name: deepseek-r1-distill-llama-8b@q4_k_m # Model name, requires pre-download from community
    url: http://localhost:1234/v1 # LM Studio service address
    api_key: lm-studio # LM Studio service fixed API Key
  
  HomeAssistant:
    # Define LLM API type
    type: homeassistant
    base_url: http://homeassistant.local:8123
    agent_id: conversation.chatgpt
    api_key: your_home_assistant_api_access_token
  
  FastgptLLM:
    # Define LLM API type
    type: fastgpt
    # If using FastGPT, config file prompt is invalid, set prompt in FastGPT console
    base_url: https://host/api/v1
    # Find your api_key here: https://cloud.tryfastgpt.ai/account/apikey
    api_key: your_fastgpt_key
    variables:
      k: "v"
      k2: "v2"
  
  XinferenceLLM:
    # Define LLM API type
    type: xinference
    # Xinference service address and model name
    model_name: qwen2.5:72b-AWQ  # Model name, requires pre-start in Xinference
    base_url: http://localhost:9997  # Xinference service address
  
  XinferenceSmallLLM:
    # Define lightweight LLM API type for intent recognition
    type: xinference
    # Xinference service address and model name
    model_name: qwen2.5:3b-AWQ  # Small model name for intent recognition
    base_url: http://localhost:9997  # Xinference service address

# VLLM configuration (Vision Language Large Model)
VLLM:
  ChatGLMVLLM:
    type: openai
    # glm-4v-flash is Zhipu AI free vision model, requires API key
    # Find your api key here: https://bigmodel.cn/usercenter/proj-mgmt/apikeys
    model_name: glm-4v-flash  # Zhipu AI vision model
    url: https://open.bigmodel.cn/api/paas/v4/
    api_key: your_api_key
  
  QwenVLVLLM:
    type: openai
    model_name: qwen2.5-vl-3b-instruct
    url: https://dashscope.aliyuncs.com/compatible-mode/v1
    # Find your api key here: https://bailian.console.aliyun.com/?apiKey=1#/api-key
    api_key: your_api_key
  
  XunfeiSparkLLM:
    # Define LLM API type
    type: openai
    # First create application at following address
    # Application: https://console.xfyun.cn/app/myapp
    # Has free quota, but must activate service to get api_key
    # Each model needs separate activation, each model has different api_password, e.g. Lite model at https://console.xfyun.cn/services/cbm
    base_url: https://ark.cn-beijing.volces.com/api/v3
    model_name: lite
    api_key: your_api_password

TTS:
  # Currently supported types: edge, doubao, can self-adapt
  EdgeTTS:
    # Define TTS API type
    type: edge
    voice: zh-CN-XiaoxiaoNeural
    output_dir: tmp/
  
  DoubaoTTS:
    # Define TTS API type
    type: doubao
    # Volcengine speech synthesis service, requires app creation and appid/access_token
    # Volcengine Speech must purchase, starting at 30 yuan for 100 concurrency. Free tier only 2 concurrent may cause TTS errors
    # After purchasing service and free voices, may need to wait ~30 minutes before use
    # Regular voices: https://console.volcengine.com/speech/service/8
    # Taiwan Xiaohe voice: https://console.volcengine.com/speech/service/10007, after activation set voice to zh_female_wanwanxiaohe_moon_bigtts
    api_url: https://openspeech.bytedance.com/api/v1/tts
    voice: BV001_streaming
    output_dir: tmp/
    authorization: "Bearer;"
    appid: your_volcengine_speech_synthesis_service_appid
    access_token: your_volcengine_speech_synthesis_service_access_token
    cluster: volcano_tts
    speed_ratio: 1.0
    volume_ratio: 1.0
    pitch_ratio: 1.0
  
  # Volcengine TTS, supports bidirectional streaming TTS
  HuoshanDoubleStreamTTS:
    type: huoshan_double_stream
    # Visit https://console.volcengine.com/speech/service/10007 to activate speech synthesis large model, purchase voice
    # Get appid and access_token at bottom of page
    # Resource ID fixed: volc.service_type.10029 (large model speech synthesis and mixing)
    # If using Gizwits Cloud, change interface address to wss://bytedance.gizwitsapi.com/api/v3/tts/bidirection
    # Gizwits Cloud doesn't need appid
    ws_url: wss://openspeech.bytedance.com/api/v3/tts/bidirection
    appid: your_volcengine_speech_synthesis_service_appid
    access_token: your_volcengine_speech_synthesis_service_access_token
    resource_id: volc.service_type.10029
    speaker: zh_female_wanwanxiaohe_moon_bigtts
    speech_rate: 0
    loudness_rate: 0
    pitch: 0
  
  CosyVoiceSiliconflow:
    type: siliconflow
    # Siliconflow TTS
    # Token application: https://cloud.siliconflow.cn/account/ak
    model: FunAudioLLM/CosyVoice2-0.5B
    voice: FunAudioLLM/CosyVoice2-0.5B:alex
    output_dir: tmp/
    access_token: your_siliconflow_api_key
    response_format: wav
  
  CozeCnTTS:
    type: cozecn
    # COZECN TTS
    # Token application: https://www.coze.cn/open/oauth/pats
    voice: 7426720361733046281
    output_dir: tmp/
    access_token: your_coze_web_key
    response_format: wav
  
  VolcesAiGatewayTTS:
    type: openai
    # Volcengine - Edge Large Model Gateway
    # First activate service, create gateway access key, search and check Doubao-TTS, activate
    # If need to use edge large model gateway LLM, also check Doubao-pro-32k-functioncall, see LLM.VolcesAiGatewayLLM configuration
    # https://console.volcengine.com/vei/aigateway/
    # After activation, get key here: https://console.volcengine.com/vei/aigateway/tokens-list
    api_key: your_gateway_access_key
    api_url: https://ai-gateway.vei.volces.com/v1/audio/speech
    model: doubao-tts
    # Voice list: https://www.volcengine.com/docs/6561/1257544
    voice: zh_male_shaonianzixin_moon_bigtts
    speed: 1
    output_dir: tmp/
  
  FishSpeech:
    # Tutorial: https://github.com/xinnan-tech/xiaozhi-esp32-server/blob/main/docs/fish-speech-integration.md
    type: fishspeech
    output_dir: tmp/
    response_format: wav
    reference_id: null
    reference_audio: ["config/assets/wakeup_words.wav",]
    reference_text: ["Hello, I'm Xiaozhi, a Taiwanese girl with a nice voice. So happy to meet you! What have you been up to lately? Don't forget to share something interesting with me, I love gossip!",]
    normalize: true
    max_new_tokens: 1024
    chunk_length: 200
    top_p: 0.7
    repetition_penalty: 1.2
    temperature: 0.7
    streaming: false
    use_memory_cache: "on"
    seed: null
    channels: 1
    rate: 44100
    api_key: "your_api_key"
    api_url: "http://127.0.0.1:8080/v1/tts"
  
  GPT_SOVITS_V2:
    # Define TTS API type
    # Start TTS method:
    # python api_v2.py -a 127.0.0.1 -p 9880 -c GPT_SoVITS/configs/demo.yaml
    type: gpt_sovits_v2
    url: "http://127.0.0.1:9880/tts"
    output_dir: tmp/
    text_lang: "auto"
    ref_audio_path: "demo.wav"
    prompt_text: ""
    prompt_lang: "zh"
    top_k: 5
    top_p: 1
    temperature: 1
    text_split_method: "cut0"
    batch_size: 1
    batch_threshold: 0.75
    split_bucket: true
    return_fragment: false
    speed_factor: 1.0
    streaming_mode: false
    seed: -1
    parallel_infer: true
    repetition_penalty: 1.35
    aux_ref_audio_paths: []
  
  GPT_SOVITS_V3:
    # Define TTS API type GPT-SoVITS-v3lora-20250228
    # Start TTS method:
    # python api.py
    type: gpt_sovits_v3
    url: "http://127.0.0.1:9880"
    output_dir: tmp/
    text_language: "auto"
    refer_wav_path: "caixukun.wav"
    prompt_language: "zh"
    prompt_text: ""
    top_k: 15
    top_p: 1.0
    temperature: 1.0
    cut_punc: ""
    speed: 1.0
    inp_refs: []
    sample_steps: 32
    if_sr: false
  
  MinimaxTTSHTTPStream:
  # Minimax streaming speech synthesis service
    type: minimax_httpstream
    output_dir: tmp/
    group_id: your_minimax_platform_group_id
    api_key: your_minimax_platform_interface_key
    model: "speech-01-turbo"
    voice_id: "female-shaonv"
    # Following optional, use default settings
    # voice_setting:
    #     voice_id: "male-qn-qingse"
    #     speed: 1
    #     vol: 1
    #     pitch: 0
    #     emotion: "happy"
    # pronunciation_dict:
    #     tone:
    #       - "处理/(chu3)(li3)"
    #       - "危险/dangerous"
    # audio_setting:
    #     sample_rate: 24000
    #     bitrate: 128000
    #     format: "mp3"
    #     channel: 1
    # timber_weights:
    #   -
    #     voice_id: male-qn-qingse
    #     weight: 1
    #   -
    #     voice_id: female-shaonv
    #     weight: 1
    # language_boost: auto
  
  AliyunTTS:
    # Aliyun Intelligent Speech Interaction Service, requires platform activation and credentials
    # Platform: https://nls-portal.console.aliyun.com/
    # Appkey: https://nls-portal.console.aliyun.com/applist
    # Token: https://nls-portal.console.aliyun.com/overview
    # Define TTS API type
    type: aliyun
    output_dir: tmp/
    appkey: your_aliyun_intelligent_speech_service_project_appkey
    token: your_aliyun_intelligent_speech_service_accesstoken_temporary_24h_or_use_below_access_key_id_access_key_secret
    voice: xiaoyun
    access_key_id: your_aliyun_account_access_key_id
    access_key_secret: your_aliyun_account_access_key_secret
    # Following optional, use default settings
    # format: wav
    # sample_rate: 16000
    # volume: 50
    # speech_rate: 0
    # pitch_rate: 0
  
  AliyunStreamTTS:
    # Aliyun CosyVoice large model streaming text-to-speech synthesis
    # Uses FlowingSpeechSynthesizer interface, supports lower latency and more natural speech quality
    # Streaming TTS only available in commercial version, no trial. See trial vs commercial. Must activate commercial version for use.
    # Supports Long series exclusive voices: longxiaochun, longyu, longchen, etc.
    # Platform: https://nls-portal.console.aliyun.com/
    # Appkey: https://nls-portal.console.aliyun.com/applist
    # Token: https://nls-portal.console.aliyun.com/overview
    # Uses three-stage streaming interaction: StartSynthesis -> RunSynthesis -> StopSynthesis
    type: aliyun_stream
    output_dir: tmp/
    appkey: your_aliyun_intelligent_speech_service_project_appkey
    token: your_aliyun_intelligent_speech_service_accesstoken_temporary_24h_or_use_below_access_key_id_access_key_secret
    voice: longxiaochun 
    access_key_id: your_aliyun_account_access_key_id
    access_key_secret: your_aliyun_account_access_key_secret
    # As of July 21, 2025, large model voices only available in Beijing node, other nodes not yet supported
    host: nls-gateway-cn-beijing.aliyuncs.com
    # Following optional, use default settings
    # format: pcm  # Audio format: pcm, wav, mp3
    # sample_rate: 16000  # Sample rate: 8000, 16000, 24000
    # volume: 50  # Volume: 0-100
    # speech_rate: 0  # Speech rate: -500 to 500
    # pitch_rate: 0  # Pitch: -500 to 500
  
  TencentTTS:
    # Tencent Cloud Intelligent Speech Interaction Service, requires platform service activation
    # appid, secret_id, secret_key application: https://console.cloud.tencent.com/cam/capi
    # Free resources: https://console.cloud.tencent.com/tts/resourcebundle
    type: tencent
    output_dir: tmp/
    appid: your_tencent_cloud_appid
    secret_id: your_tencent_cloud_secretid
    secret_key: your_tencent_cloud_secretkey
    region: ap-guangzhou
    voice: 101001

  TTS302AI:
    # 302AI speech synthesis service, requires 302 platform account recharge and key
    # Add 302.ai TTS configuration
    # Token application: https://dash.302.ai/
    # Get api_key path: https://dash.302.ai/apis/list
    # Price: $35/million characters. Volcengine original ¥450/million characters
    type: doubao
    api_url: https://api.302ai.cn/doubao/tts_hd
    authorization: "Bearer "
    # Taiwan Xiaohe voice
    voice: "zh_female_wanwanxiaohe_moon_bigtts"
    output_dir: tmp/
    access_token: "your_302_api_key"
  
  GizwitsTTS:
    type: doubao
    # Volcengine as base, can fully use enterprise-level Volcengine speech synthesis service
    # First 10,000 registered users receive 5 yuan credit
    # Get API Key: https://agentrouter.gizwitsapi.com/panel/token
    api_url: https://bytedance.gizwitsapi.com/api/v1/tts
    authorization: "Bearer "
    # Taiwan Xiaohe voice
    voice: "zh_female_wanwanxiaohe_moon_bigtts"
    output_dir: tmp/
    access_token: "your_gizwits_cloud_api_key"
  
  ACGNTTS:
    # Online website: https://acgn.ttson.cn/
    # Token purchase: www.ttson.cn
    # Development questions submit to QQ on website
    # Character ID retrieval: ctrl+f quick search character - website admin doesn't allow publishing, can ask admin
    # Parameter meanings see development docs: https://www.yuque.com/alexuh/skmti9/wm6taqislegb02gd?singleDoc#
    type: ttson
    token: your_token
    voice_id: 1695
    speed_factor: 1
    pitch_factor: 0
    volume_change_dB: 0
    to_lang: ZH
    url: https://u95167-bd74-2aef8085.westx.seetacloud.com:8443/flashsummary/tts?token=
    format: mp3
    output_dir: tmp/
    emotion: 1
  
  OpenAITTS:
    # OpenAI official text-to-speech service, supports most global languages
    type: openai
    # Get api key here: https://platform.openai.com/api-keys
    api_key: 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
    # Requires proxy in China
    api_url: https://api.openai.com/v1/audio/speech
    # Options: tts-1 or tts-1-hd, tts-1 faster, tts-1-hd better quality
    model: tts-1
    # Speaker options: alloy, echo, fable, onyx, nova, shimmer
    voice: nova
    # Speed range 0.25-4.0
    speed: 1
    output_dir: tmp/
  
  CustomTTS:
    # Custom TTS interface service, customizable request parameters, can integrate many TTS services
    # Example using locally deployed KokoroTTS
    # CPU only: docker run -p 8880:8880 ghcr.io/remsky/kokoro-fastapi-cpu:latest
    # GPU: docker run --gpus all -p 8880:8880 ghcr.io/remsky/kokoro-fastapi-gpu:latest
    # Requires POST request method and audio file return
    type: custom
    method: POST
    url: "http://127.0.0.1:8880/v1/audio/speech"
    params: # Custom request parameters
      input: "{prompt_text}"
      response_format: "mp3"
      download_format: "mp3"
      voice: "zf_xiaoxiao"
      lang_code: "z"
      return_download_link: true
      speed: 1
      stream: false
    headers: # Custom request headers
      # Authorization: Bearer xxxx
    format: mp3 # Interface returned audio format
    output_dir: tmp/
  
  LinkeraiTTS:
    type: linkerai
    api_url: https://tts.linkerai.cn/tts
    audio_format: "pcm"
    # Default access_token is free for testing, do not use for commercial purposes
    # If results are good, apply for own token: https://linkerai.cn
    # Parameter meanings see development docs: https://tts.linkerai.cn/docs
    # Supports voice cloning, can upload audio and fill in voice parameter, empty voice uses default voice
    access_token: "U4YdYXVfpwWnk2t5Gp822zWPCuORyeJL"
    voice: "OUeAo1mhq6IBExi"
    output_dir: tmp/
  
  PaddleSpeechTTS:
    # Baidu PaddlePaddle PaddleSpeech supports local offline deployment and model training
    # Framework: https://www.paddlepaddle.org.cn/
    # Project: https://github.com/PaddlePaddle/PaddleSpeech
    # SpeechServerDemo: https://github.com/PaddlePaddle/PaddleSpeech/tree/develop/demos/speech_server
    # Streaming reference: https://github.com/PaddlePaddle/PaddleSpeech/wiki/PaddleSpeech-Server-WebSocket-API
    type: paddle_speech
    protocol: websocket # protocol choices = ['websocket', 'http']
    url: ws://127.0.0.1:8092/paddlespeech/tts/streaming  # TTS service URL, points to local server [websocket default ws://127.0.0.1:8092/paddlespeech/tts/streaming, http default http://127.0.0.1:8090/paddlespeech/tts]
    spk_id: 0  # Speaker ID, 0 usually default speaker
    sample_rate: 24000  # Sample rate [websocket default 24000, http default 0 auto select]
    speed: 1.0  # Speed, 1.0 normal, >1 faster, <1 slower
    volume: 1.0  # Volume, 1.0 normal, >1 louder, <1 quieter
    save_path:   # Save path
  
  IndexStreamTTS:
    # TTS interface service based on Index-TTS-vLLM project
    # Tutorial: https://github.com/Ksuriuri/index-tts-vllm/blob/master/README.md
    type: index_stream
    api_url: http://127.0.0.1:11996/tts
    audio_format: "pcm"
    # Default voice, for other voices register in project assets folder
    voice: "jay_klee"
    output_dir: tmp/
  
  AliBLTTS:
    # Aliyun Bailian CosyVoice large model streaming text-to-speech synthesis
    # Find your api_key here: https://bailian.console.aliyun.com/?apiKey=1#/api-key
    # cosyvoice-v3 and some voices require application activation
    type: alibl_stream
    api_key: your_api_key
    model: "cosyvoice-v2"
    voice: "longcheng_v2"
    output_dir: tmp/
    # Following optional, use default settings
    # format: pcm  # Audio format: pcm, wav, mp3, opus
    # sample_rate: 24000  # Sample rate: 16000, 24000, 48000
    # volume: 50  # Volume: 0-100
    # rate: 1  # Speech rate: 0.5~2
    # pitch: 1  # Pitch: 0.5~2
  
  XunFeiTTS:
    # iFlytek TTS service Official website: https://www.xfyun.cn/
    # Login to iFlytek Speech Technology Platform https://console.xfyun.cn/app/myapp to create application
    # Select needed service to get api configuration https://console.xfyun.cn/services/uts
    # Purchase service for application (APPID) e.g.: Ultra-realistic synthesis https://console.xfyun.cn/services/uts
    type: xunfei_stream
    api_url: wss://cbm01.cn-huabei-1.xf-yun.com/v1/private/mcd9m97e6
    app_id: your_app_id
    api_secret: your_api_secret
    api_key: your_api_key
    voice: x5_lingxiaoxuan_flow
    output_dir: tmp/
    # Following optional, use default settings, note V5 voices don't support colloquialization config
    # oral_level: mid  # Colloquialization level: high, mid, low
    # spark_assist: 1  # Whether to use large model for colloquialization Enable:1, Disable:0
    # stop_split: 0  # Disable server-side sentence splitting Not disabled: 0, Disabled: 1
    # remain: 0  # Whether to preserve original written form Preserve:1, Don't preserve:0
    # format: raw  # Audio format: raw(PCM), lame(MP3), speex, opus, opus-wb, opus-swb, speex-wb
    # sample_rate: 24000  # Sample rate: 16000, 8000, 24000
    # volume: 50  # Volume: 0-100
    # speed: 50  # Speed: 0-100
    # pitch: 50  # Pitch: 0-100
